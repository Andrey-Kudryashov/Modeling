{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import nessesary libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel('Task_Python.xlsx', sheet_name='Train')\n",
    "df_test = pd.read_excel('Task_Python.xlsx', sheet_name='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = df_train.drop(['y'], axis=1)\n",
    "y_train_pre = np.where(df_train.y == 'yes', 1, 0)\n",
    "X_test_pre = df_test\n",
    "columns = X_train_pre.columns\n",
    "# We will use list of categorical variables later\n",
    "categorical_features = []\n",
    "is_categorical = X_train_pre.dtypes == 'object'\n",
    "for col in X_train_pre.columns:\n",
    "    if is_categorical[col]: categorical_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_pre.reset_index(drop=True)\n",
    "y_train = y_train_pre\n",
    "# Encoding for categorical variables, as most models don't work with strings\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "# for feature in categorical_features:\n",
    "#     le = LabelEncoder()\n",
    "#     X_train[feature]  = le.fit_transform(X_train[feature])\n",
    "# one = OneHotEncoder(categorical_features=[X_train.columns.get_loc(i) for i in categorical_features])\n",
    "# X_train = one.fit_transform(X_train).toarray()\n",
    "# Standartization, as most models are sensetive to magnitude of variables\n",
    "Scaler = StandardScaler(with_mean=False)\n",
    "# columns = X_train.columns\n",
    "X_train = Scaler.fit_transform(X_train)\n",
    "# Our data is imbalanced (1:9). So, we use SMOTE to synthesize more class 1 observations\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "# X_train = pd.DataFrame(X_train, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modeling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression: 0.8947402815778542 (+/- 0.010602669525754749)\n",
      "SVC: 0.9490280004599647 (+/- 0.0076458285694324364)\n",
      "LDA: 0.8857006685167738 (+/- 0.009350425909110331)\n",
      "QDA: 0.7027055312386444 (+/- 0.044288040299362)\n",
      "RF: 0.9518037213153834 (+/- 0.0051253018072496)\n",
      "GBC: 0.941323428386337 (+/- 0.005655493033103703)\n",
      "XGB: 0.9420774122464783 (+/- 0.007277938673366541)\n",
      "LGBM: 0.9553533492004276 (+/- 0.0067944646931446875)\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "            (\"Logistic_Regression\", LogisticRegression()),\n",
    "            (\"SVC\", SVC()),\n",
    "            (\"LDA\", LinearDiscriminantAnalysis()),\n",
    "            (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "            (\"RF\", RandomForestClassifier()),\n",
    "            (\"GBC\", GradientBoostingClassifier()),\n",
    "            (\"XGB\", XGBClassifier()),\n",
    "            (\"LGBM\", LGBMClassifier())\n",
    "        ]  \n",
    "    \n",
    "scoring = 'f1'\n",
    "n_folds = 5\n",
    "results, names  = [], [] \n",
    "\n",
    "for name, model  in model_list:\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv= kfold,\n",
    "                                 scoring=scoring, n_jobs=-1)    \n",
    "    names.append(name)\n",
    "    results.append(cv_results)    \n",
    "    print(f'{name}: {cv_results.mean()} (+/- {cv_results.std()})')\n",
    "    \n",
    "results = pd.DataFrame(np.array(results).T, columns = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Cat Boosting</h5><br>This model has its own specific requirements (categotical variables can be left alone and passed to a model without encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_pre.reset_index(drop=True)\n",
    "y_train = y_train_pre\n",
    "for col in X_train.columns:\n",
    "    if col not in categorical_features:\n",
    "        Scaler = StandardScaler()\n",
    "        X_train[col] = Scaler.fit_transform(X_train[col].values.reshape(-1,1))\n",
    "\n",
    "smotenc = SMOTENC(categorical_features=[X_train.columns.get_loc(i) for i in categorical_features])\n",
    "X_train, y_train = smotenc.fit_sample(X_train, y_train)\n",
    "X_train = pd.DataFrame(X_train, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat_Boosting: 0.9415684137438074 (+/- 0.005140694316555464)\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(cat_features=categorical_features)\n",
    "\n",
    "scoring = 'f1'\n",
    "n_folds = 5\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "cv_results = cross_val_score(model, X_train, y_train, cv= kfold,\n",
    "                             scoring=scoring, n_jobs=-1)    \n",
    "\n",
    "print(f'Cat_Boosting: {cv_results.mean()} (+/- {cv_results.std()})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regularization of the best model</h3><h5>Light gradient boosting machine hyperparameters tuning</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   18.8s\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   45.2s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:  2.8min\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  3.1min\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  4.5min\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:  4.9min\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  5.8min\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=4)]: Done 243 out of 243 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Best score----------\n",
      "0.9542962660072529\n",
      "-------Best params----------\n",
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300, 'num_leaves': 200}\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier()\n",
    "\n",
    "param_grid_et =  {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'num_leaves': [200, 500, 900]\n",
    "               }\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=model,\n",
    "                               param_grid= param_grid_et,\n",
    "                               scoring= 'f1',\n",
    "                               cv = StratifiedKFold(n_splits=3, shuffle=True),\n",
    "                               n_jobs = 4,\n",
    "                               verbose = 10)\n",
    "# Fit grid\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best score and best parameters\n",
    "print('-------Best score----------')\n",
    "print(grid_rf.best_score_ )\n",
    "print('-------Best params----------')\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction</h1><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_pre.reset_index(drop=True)\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_test[feature]  = le.fit_transform(X_test[feature])\n",
    "X_test = pd.DataFrame(Scaler.transform(X_test), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final model\n",
    "model = LGBMClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=15,\n",
    "    n_estimators=300,\n",
    "    num_leaves=200\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
